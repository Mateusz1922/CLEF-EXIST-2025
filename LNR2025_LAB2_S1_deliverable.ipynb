{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVCyAh4-MWgx"
      },
      "source": [
        "<h1 align=\"center\">Lab 2:  Sexism Identification in Twitter</h1>\n",
        "<h2 align=\"center\">Session 1. Machine Learning and Feature Engineering</h2>\n",
        "\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Polit√®cnica de Val√®ncia</h3>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40QF5NzZMWg1"
      },
      "source": [
        "### Put your names here\n",
        "\n",
        "- Mateusz Kr√≥l\n",
        "- (replace with another name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Ealvg3nM0Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uexir0NmwwBi"
      },
      "outputs": [],
      "source": [
        "# Reading the entire dataset for both languages and considering only the hard labels. In this lab we do not address the sexism identification task from a Learning with Disagreement (LwD) perspective.\n",
        "\n",
        "from readerEXIST2025 import EXISTReader\n",
        "\n",
        "reader_train = EXISTReader(\"EXIST2025_training.json\")\n",
        "reader_dev = EXISTReader(\"EXIST2025_dev.json\")\n",
        "\n",
        "EnTrainTask1, EnDevTask1 = reader_train.get(lang=\"EN\", subtask=\"1\"), reader_dev.get(lang=\"EN\", subtask=\"1\")\n",
        "EnTrainTask2, EnDevTask2 = reader_train.get(lang=\"EN\", subtask=\"2\"), reader_dev.get(lang=\"EN\", subtask=\"2\")\n",
        "\n",
        "SpTrainTask1, SpDevTask1 = reader_train.get(lang=\"ES\", subtask=\"1\"), reader_dev.get(lang=\"ES\", subtask=\"1\")\n",
        "SpTrainTask2, SpDevTask2 = reader_train.get(lang=\"ES\", subtask=\"2\"), reader_dev.get(lang=\"ES\", subtask=\"2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(reader_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqp-NIX6PGOp",
        "outputId": "0e6675ba-47f0-4bde-e577-51b7699e1cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'readerEXIST2025.EXISTReader'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader_train.get(lang=\"EN\", subtask=\"2\")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "dqhjRKlNVM77",
        "outputId": "67ff6607-47fd-4d9c-beb0-8c13614b131e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1       Writing a uni essay in my local pub with a cof...\n",
              "2       @UniversalORL it is 2021 not 1921. I dont appr...\n",
              "5       According to a customer I have plenty of time ...\n",
              "6       So only 'blokes' drink beer? Sorry, but if you...\n",
              "10      #EverydaySexism means women usually end up in ...\n",
              "                              ...                        \n",
              "3253    @ShefVaidya Ma'am if I say that you look like ...\n",
              "3255    idk why y‚Äôall bitches think having half your a...\n",
              "3256    This has been a part of an experiment with @Wo...\n",
              "3257    \"Take me already\" \"Not yet. You gotta be ready...\n",
              "3258    @clintneedcoffee why do you look like a whore?...\n",
              "Name: text, Length: 856, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>According to a customer I have plenty of time ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>#EverydaySexism means women usually end up in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3253</th>\n",
              "      <td>@ShefVaidya Ma'am if I say that you look like ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3255</th>\n",
              "      <td>idk why y‚Äôall bitches think having half your a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3256</th>\n",
              "      <td>This has been a part of an experiment with @Wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3257</th>\n",
              "      <td>\"Take me already\" \"Not yet. You gotta be ready...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3258</th>\n",
              "      <td>@clintneedcoffee why do you look like a whore?...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>856 rows √ó 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrbD5WxJD3eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgRfO0_UMWg2"
      },
      "source": [
        "# ENGLISH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GZPxeJcMWg3"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "BpoNfcxqZFUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsBKtO3HMWg3"
      },
      "outputs": [],
      "source": [
        "# COMPLETE IF YOU WANT TO DO TEXT PREPROCESSING\n",
        "\n",
        "web_re = re.compile(r\"https?:\\/\\/[^\\s]+\", re.U)\n",
        "user_re = re.compile(r\"(@\\w+\\-?(?:\\w+)?)\", re.U)\n",
        "hashtag_re = re.compile(r\"(#\\w+\\-?(?:\\w+)?)\", re.U)\n",
        "\n",
        "def preprocess(text):\n",
        "    # COMPLETE\n",
        "    text = pd.Series(text)\n",
        "    text = text.str.replace(user_re, \"\", regex=True)\n",
        "    text = text.str.replace(web_re, \"\", regex=True)\n",
        "    text = text.str.replace(hashtag_re, \"\", regex=True).str.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(EnTrainTask1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvYdMvHzaWUq",
        "outputId": "55b8d570-6ae0-4b1f-db01-681d328bdba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1       200002\n",
            "2       200003\n",
            "5       200006\n",
            "6       200007\n",
            "7       200008\n",
            "         ...  \n",
            "3255    203256\n",
            "3256    203257\n",
            "3257    203258\n",
            "3258    203259\n",
            "3259    203260\n",
            "Name: id, Length: 2870, dtype: object, 1       Writing a uni essay in my local pub with a cof...\n",
            "2       @UniversalORL it is 2021 not 1921. I dont appr...\n",
            "5       According to a customer I have plenty of time ...\n",
            "6       So only 'blokes' drink beer? Sorry, but if you...\n",
            "7       New to the shelves this week - looking forward...\n",
            "                              ...                        \n",
            "3255    idk why y‚Äôall bitches think having half your a...\n",
            "3256    This has been a part of an experiment with @Wo...\n",
            "3257    \"Take me already\" \"Not yet. You gotta be ready...\n",
            "3258    @clintneedcoffee why do you look like a whore?...\n",
            "3259    ik when mandy says ‚Äúyou look like a whore‚Äù i l...\n",
            "Name: text, Length: 2870, dtype: object, 1       YES\n",
            "2       YES\n",
            "5       YES\n",
            "6       YES\n",
            "7        NO\n",
            "       ... \n",
            "3255    YES\n",
            "3256    YES\n",
            "3257    YES\n",
            "3258    YES\n",
            "3259    YES\n",
            "Name: label1, Length: 2870, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EnTrainTask1_new = preprocess(EnTrainTask1[1])\n",
        "EnDevTask1_new = preprocess(EnDevTask1[1])\n",
        "EnTrainTask2_new = preprocess(EnTrainTask2[1])\n",
        "EnDevTask2_new = preprocess(EnDevTask2[1])\n",
        "SpTrainTask1_new = preprocess(SpTrainTask1[1])\n",
        "SpDevTask1_new = preprocess(SpDevTask1[1])\n",
        "SpTrainTask2_new = preprocess(SpTrainTask2[1])\n",
        "SpDevTask2_new = preprocess(SpDevTask2[1])\n",
        "\n",
        "print(SpTrainTask1_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa8dTc3eZvVv",
        "outputId": "9be8fe5c-8ac0-466c-f5a9-d9c63ecde8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3260     ignora al otro, es un capullo.el problema con...\n",
            "3261     si comicsgate se parece en algo a gamergate p...\n",
            "3262     lee sobre gamergate, y como eso ha cambiado l...\n",
            "3264       entonces como as√≠ es el mercado lo mejor no...\n",
            "3265     aaah s√≠. andrew dobson. el que se dedic√≥ a ec...\n",
            "                              ...                        \n",
            "6914    que envidia me dan las dos zorras estas, yo de...\n",
            "6915    los hombres sieeeempre dicen que si fueran muj...\n",
            "6916     estas zorras qls siempre preocupadas de hasta...\n",
            "6917    a pesar de que convenga que las zorras jueguen...\n",
            "6919     aaaaaaa ¬øqui√©n te manda socializar con nosotr...\n",
            "Name: text, Length: 3194, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-xJznBkO5Z2",
        "outputId": "d1cee1db-e6b7-4c32-def1-0b795a8b55d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Preprocessing\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# def clean_text(text):\n",
        "#     text = text.lower()  # Lowercasing\n",
        "#     text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
        "#     text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "#     words = word_tokenize(text)  # Tokenization\n",
        "#     words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization & stopword removal\n",
        "#     return ' '.join(words)\n",
        "\n",
        "# # Apply to dataset\n",
        "# EnTrainTask1_cleaned = [clean_text(text) for text in EnTrainTask1[1]]\n",
        "# EnDevTask1_cleaned = [clean_text(text) for text in EnDevTask1[1]]\n",
        "\n",
        "# print(EnTrainTask1_cleaned[:5])  # See cleaned texts\n"
      ],
      "metadata": {
        "id": "AVP1ktLN_RZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39emgSdFPI8o",
        "outputId": "01747a31-47b4-4941-d676-07f10d177327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import emoji\n",
        "\n",
        "# Regular expressions for cleaning\n",
        "web_re = re.compile(r\"https?:\\/\\/[^\\s]+\", re.U)  # Remove URLs\n",
        "user_re = re.compile(r\"(@\\w+\\-?(?:\\w+)?)\", re.U)  # Remove mentions\n",
        "hashtag_re = re.compile(r\"(#\\w+\\-?(?:\\w+)?)\", re.U)  # Remove hashtags\n",
        "special_chars_re = re.compile(r\"[^a-zA-Z0-9\\s]\", re.U)  # Remove special characters\n",
        "\n",
        "def preprocess(text):\n",
        "    text = pd.Series(text)  # Convert input to Pandas Series if it's not already\n",
        "\n",
        "    text = text.str.replace(user_re, \"\", regex=True)  # Remove @mentions\n",
        "    text = text.str.replace(web_re, \"\", regex=True)  # Remove URLs\n",
        "    text = text.str.replace(hashtag_re, \"\", regex=True)  # Remove hashtags\n",
        "    text = text.apply(lambda x: emoji.replace_emoji(x, replace=\"\"))  # Remove emojis\n",
        "    text = text.str.replace(special_chars_re, \"\", regex=True)  # Remove special characters\n",
        "    text = text.str.lower().str.strip()  # Convert to lowercase and remove extra spaces\n",
        "\n",
        "    return text.tolist()  # Convert back to a list if needed\n",
        "\n",
        "# Example usage\n",
        "sample_tweets = [\n",
        "    \"@user1 OMG! This is crazy üò° #shockingnews https://t.co/xyz123\",\n",
        "    \"Women deserve equal pay! #EqualPayNow üí∞\",\n",
        "    \"Visit our website: www.example.com #ad\"\n",
        "]\n",
        "\n",
        "cleaned_tweets = preprocess(sample_tweets)\n",
        "print(cleaned_tweets)"
      ],
      "metadata": {
        "id": "Jsfw9YTGANR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b9d0b3-0b12-4aec-9d0c-2f8b3a65091b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['omg this is crazy', 'women deserve equal pay', 'visit our website wwwexamplecom']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EnTrainTask1_cleaned = [preprocess(text) for text in EnTrainTask1[1]]\n",
        "EnDevTask1_cleaned = [preprocess(text) for text in EnDevTask1[1]]\n",
        "EnTrainTask2_cleaned = [preprocess(text) for text in EnTrainTask2[1]]\n",
        "EnDevTask2_cleaned = [preprocess(text) for text in EnDevTask2[1]]\n",
        "SpTrainTask1_cleaned = [preprocess(text) for text in SpTrainTask1[1]]\n",
        "SpDevTask1_cleaned = [preprocess(text) for text in SpDevTask1[1]]\n",
        "SpTrainTask2_cleaned = [preprocess(text) for text in SpTrainTask2[1]]\n",
        "SpDevTask2_cleaned = [preprocess(text) for text in SpDevTask2[1]]"
      ],
      "metadata": {
        "id": "QsO3n3RLPUeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text representation"
      ],
      "metadata": {
        "id": "Cnv-0S3S-aUR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Gpyme-MWg3"
      },
      "source": [
        "##¬†Tweet representations (Feature extraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "uVSUc06j-6co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert everything to string and remove NaN values\n",
        "EnTrainTask1_cleaned = [str(text) for text in EnTrainTask1_cleaned if text is not None]\n",
        "EnDevTask1_cleaned = [str(text) for text in EnDevTask1_cleaned if text is not None]\n",
        "EnTrainTask2_cleaned = [str(text) for text in EnTrainTask2_cleaned if text is not None]\n",
        "EnDevTask2_cleaned = [str(text) for text in EnDevTask2_cleaned if text is not None]\n",
        "\n",
        "SpTrainTask1_cleaned = [str(text) for text in SpTrainTask1_cleaned if text is not None]\n",
        "SpDevTask1_cleaned = [str(text) for text in SpDevTask1_cleaned if text is not None]\n",
        "SpTrainTask2_cleaned = [str(text) for text in SpTrainTask2_cleaned if text is not None]\n",
        "SpDevTask2_cleaned = [str(text) for text in SpDevTask2_cleaned if text is not None]\n",
        "\n",
        "# Ensure no empty strings (optional)\n",
        "EnTrainTask1_cleaned = [text for text in EnTrainTask1_cleaned if text.strip() != \"\"]\n",
        "EnDevTask1_cleaned = [text for text in EnDevTask1_cleaned if text.strip() != \"\"]\n",
        "EnTrainTask1_cleaned = [text for text in EnTrainTask1_cleaned if text.strip() != \"\"]\n",
        "EnDevTask1_cleaned = [text for text in EnDevTask1_cleaned if text.strip() != \"\"]\n",
        "\n",
        "SpTrainTask1_cleaned = [text for text in SpTrainTask1_cleaned if text.strip() != \"\"]\n",
        "SpDevTask1_cleaned = [text for text in SpDevTask1_cleaned if text.strip() != \"\"]\n",
        "SpTrainTask2_cleaned = [text for text in SpTrainTask2_cleaned if text.strip() != \"\"]\n",
        "SpDevTask2_cleaned = [text for text in SpDevTask2_cleaned if text.strip() != \"\"]\n",
        "\n",
        "# Check again\n",
        "print(type(EnTrainTask1_cleaned[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5MrKtDsQqJ1",
        "outputId": "dbd440ae-4cae-45eb-dc60-471b21dfd1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(EnTrainTask1_cleaned[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eMcYtEc0mZc",
        "outputId": "ff1c3a1d-8076-4ae9-e102-050963198e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"['writing a uni essay in my local pub with a coffee random old man keeps asking me drunk questions when im trying to concentrate amp ends with good luck but youll just end up getting married and not use it anyway  is alive and well']\", \"['it is 2021 not 1921 i dont appreciate that on two rides by myself your team member looked behind me and asked the man behind how many in my party not impressed']\", \"['according to a customer i have plenty of time to go spent the stirling coins he wants to pay me with in derry just like any other woman im sure of it  in retail']\", \"['so only blokes drink beer sorry but if you arent a bloke you drink wine apparently  alive and well in']\", \"['new to the shelves this week  looking forward to reading these books']\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(SpTrainTask1_cleaned[:5])  # Sample Spanish tweets after preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSncDUOR0jnm",
        "outputId": "0902a93e-5b09-4606-9fc7-55971fb5acd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"['ignora al otro es un capulloel problema con este youtuber denuncia el acoso cuando no afecta a la gente de izquierdas por ejemplo en su video sobre el gamergate presenta como normal el acoso que reciben fisher anita o zey cuando hubo hasta amenazas de bomba']\", \"['si comicsgate se parece en algo a gamergate pues muy bien por el acoso y si se est haciendo un sabotaje porque hay personajes que no os gustan entonces gracias por darme la razn sois unos lloricas ofendidos']\", \"['lee sobre gamergate y como eso ha cambiado la manera en la cual nos comunicamos en el internet los fanboys de halo estn txicos pero los fanboys de otras comunidadesjuegos tambin han querido coger pauta con eso']\", \"['entonces como as es el mercado lo mejor no es hacer algo para cambiarlo y seguir alimentando el machismo en los consumidores en lugar apoyar a gente como las vctimas del gamergateacerca de lo otro el tenan implica un imperativo entonces no entiendo lo del buscaban']\", \"['aaah s andrew dobson el que se dedic a echar mierda del gamergate al ms puro estilo white knight y todo deviantart se le ech encima']\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert labels to numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(EnTrainTask1[2])  # Binary labels\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(EnTrainTask1_cleaned)  # Training text data"
      ],
      "metadata": {
        "id": "Zndb0mQI-2Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(EnTrainTask1_cleaned))  # Should be 2870\n",
        "# print(len(y_train))  # Should also be 2870"
      ],
      "metadata": {
        "id": "msntu5wVQVS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to numerical format\n",
        "label_encoder2 = LabelEncoder()\n",
        "y_train2 = label_encoder2.fit_transform(EnTrainTask2[2])  # Binary labels\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer2 = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf2 = vectorizer2.fit_transform(EnTrainTask2_cleaned)  # Training text data\n",
        "\n",
        "X_dev_tfidf2 = vectorizer2.transform(EnDevTask2_cleaned)\n",
        "y_dev2 = label_encoder2.transform(EnDevTask2[2])"
      ],
      "metadata": {
        "id": "HwmVl5OZ_BSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "pAPyVmYl-zrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim"
      ],
      "metadata": {
        "id": "4wzNgSkH073u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# nlp = spacy.load(\"es_core_news_md\")  # Load Spanish model\n",
        "\n",
        "# def get_embedding(text):\n",
        "#     return nlp(text).vector  # Extract dense vector representation\n",
        "\n",
        "# X_train_emb = [get_embedding(text) for text in SpTrainTask1_cleaned]\n",
        "# X_dev_emb = [get_embedding(text) for text in SpDevTask1_cleaned]\n"
      ],
      "metadata": {
        "id": "cwakY94S1pxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0Bhgk0pMWg4"
      },
      "outputs": [],
      "source": [
        "# # Obtaining a representation for the train and dev subsets in both tasks\n",
        "\n",
        "# # COMPLETE\n",
        "# # import gensim\n",
        "# from gensim.models import Word2Vec\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# # Tokenize sentences\n",
        "# tokenized_texts = [word_tokenize(text.lower()) for text in SpTrainTask1_cleaned]\n",
        "\n",
        "# # Train Word2Vec model\n",
        "# w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "# # Convert text data to vectors\n",
        "# def get_avg_word_vector(text, model, vector_size):\n",
        "#     words = word_tokenize(text.lower())\n",
        "#     vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "#     return sum(vectors) / len(vectors) if vectors else [0] * vector_size\n",
        "\n",
        "# X_train_w2v = [get_avg_word_vector(text, w2v_model, 100) for text in SpTrainTask1_cleaned]\n",
        "# X_test_w2v = [get_avg_word_vector(text, w2v_model, 100) for text in SpDevTask1[1]]\n",
        "\n",
        "# print(len(X_train_w2v), len(X_train_w2v[0]))  # Output: (num_samples, 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR1AwmUwMWg4"
      },
      "source": [
        "## Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Train Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on development set\n",
        "X_dev_tfidf = vectorizer.transform(EnDevTask1_cleaned)\n",
        "y_dev = label_encoder.transform(EnDevTask1[2])\n",
        "\n",
        "y_pred = lr.predict(X_dev_tfidf)\n",
        "\n",
        "f1_positive\t= f1_score(y_dev, y_pred, pos_label=1)\n",
        "print(\"Subtask 1\")\n",
        "print(f\"F1-score (Positive\tClass):\t{f1_positive}\")\n",
        "print(classification_report(y_dev, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLxWyXqKTGBU",
        "outputId": "b5ad9985-ef4e-44d8-816e-c79f8f65494e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 1\n",
            "F1-score (Positive\tClass):\t0.6827794561933535\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.90      0.81       250\n",
            "           1       0.82      0.58      0.68       194\n",
            "\n",
            "    accuracy                           0.76       444\n",
            "   macro avg       0.78      0.74      0.75       444\n",
            "weighted avg       0.77      0.76      0.76       444\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression\n",
        "lr2 = LogisticRegression()\n",
        "lr2.fit(X_train_tfidf2, y_train2)\n",
        "\n",
        "# Predict on development set\n",
        "X_dev_tfidf2 = vectorizer2.transform(EnDevTask2_cleaned)\n",
        "y_dev2 = label_encoder2.transform(EnDevTask2[2])\n",
        "\n",
        "y_pred2 = lr2.predict(X_dev_tfidf2)\n",
        "\n",
        "f1_positive\t= f1_score(y_dev2, y_pred2, average = 'macro')\n",
        "print(\"Subtask 2\")\n",
        "print(f\"F1-score (Positive\tClass):\t{f1_positive}\")\n",
        "print(classification_report(y_dev2, y_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_pCqPPxTw_M",
        "outputId": "cbc5e835-034d-40a5-80a3-30b35e2b2220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 2\n",
            "F1-score (Positive\tClass):\t0.32091290172333486\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75        85\n",
            "           1       0.00      0.00      0.00        28\n",
            "           2       0.80      0.12      0.21        33\n",
            "\n",
            "    accuracy                           0.61       146\n",
            "   macro avg       0.47      0.37      0.32       146\n",
            "weighted avg       0.53      0.61      0.49       146\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "###########################SUBTASK\t1##################################\n",
        "#Using\tthe\tSVC\twith\tdefault\tsettings\n",
        "clf1_en\t= svm.SVC()\n",
        "clf1_en.fit(X_train_tfidf, y_train)\n",
        "predicted1 = clf1_en.predict(X_dev_tfidf)\n",
        "# print(\"Predicted task1\\t-->\\t\",\tpredicted1)\n",
        "# print(\"GroundTruth task1\\t-->\\t\", y_dev)\n",
        "#Computing\tthe\tF1\tScore\tfor\tthe\tpositive\tclass\t(sexism),\tsubtask\t1\n",
        "print(\"Subtask 1\")\n",
        "f1_positive\t= f1_score(y_dev,\tpredicted1,\tpos_label=1)\n",
        "print(f\"F1-score (Positive\tClass):\t{f1_positive}\")\n",
        "#Obtainig a detailed classification\treport\n",
        "report\t= classification_report(y_dev, predicted1, digits=4)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_mf8pebCHC8",
        "outputId": "5db5297f-84cd-4755-ef2b-1a30096676b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 1\n",
            "F1-score (Positive\tClass):\t0.6966966966966966\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7443    0.9080    0.8180       250\n",
            "           1     0.8345    0.5979    0.6967       194\n",
            "\n",
            "    accuracy                         0.7725       444\n",
            "   macro avg     0.7894    0.7530    0.7574       444\n",
            "weighted avg     0.7837    0.7725    0.7650       444\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################SUBTASK\t2##################################\n",
        "clf2_en\t= svm.SVC()\n",
        "clf2_en.fit(X_train_tfidf2, y_train2)\n",
        "predicted2 = clf2_en.predict(X_dev_tfidf2)\n",
        "# print(\"Predicted task2\\t-->\\t\",\tpredicted2)\n",
        "# print(\"GroundTruth task2\\t-->\\t\", EnDevTask2[2])\n",
        "#Computing\tthe\tmacro-average F1 for the second\tsubtask\n",
        "f1_macro = f1_score(y_dev2, predicted2,\taverage='macro')\n",
        "print(\"Subtask 2\")\n",
        "print(f\"F1-score (Macro-Averaged): {f1_macro}\")\n",
        "#Obtainig a detailed classification\treport\n",
        "report = classification_report(y_dev2, predicted2, digits=4)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSpj9cPfLEh6",
        "outputId": "09bd025c-2693-4acb-af29-9dfd0d7dd9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 2\n",
            "F1-score (Macro-Averaged): 0.24531024531024528\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5822    1.0000    0.7359        85\n",
            "           1     0.0000    0.0000    0.0000        28\n",
            "           2     0.0000    0.0000    0.0000        33\n",
            "\n",
            "    accuracy                         0.5822       146\n",
            "   macro avg     0.1941    0.3333    0.2453       146\n",
            "weighted avg     0.3389    0.5822    0.4285       146\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSVddJgGMWg4"
      },
      "outputs": [],
      "source": [
        "# COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network\timport MLPClassifier\n",
        "from sklearn.metrics\timport f1_score,\tclassification_report\n",
        "###########################SUBTASK\t1##################################\n",
        "#Using\tthe\tMLP\twith\tdefault\tsettings\n",
        "clf1_en\t=\tMLPClassifier(random_state=1,\tmax_iter=300)\n",
        "clf1_en.fit(X_train_tfidf,\ty_train)\n",
        "predicted1\t=\tclf1_en.predict(X_dev_tfidf)\n",
        "# print(\"Predicted\ttask1\\t-->\\t\",\tpredicted1)\n",
        "# print(\"GroundTruth\ttask1\\t-->\\t\",\ty1_dev)\n",
        "#Computing\tthe\tF1\tScore\tfor\tthe\tpositive\tclass\t(sexism),\tsubtask\t1\n",
        "f1_positive\t=\tf1_score(y_dev,\tpredicted1,\tpos_label=1)\n",
        "print(\"Subtask 1\")\n",
        "print(f\"F1-score\t(Positive\tClass):\t{f1_positive}\")\n",
        "#Obtainig\ta\tdetailed\tclassification\treport\n",
        "report\t=\tclassification_report(y_dev, predicted1,\tdigits=4)\n",
        "print(report)\n",
        "###########################SUBTASK\t2##################################\n",
        "clf2_en\t=\tMLPClassifier(random_state=1,\tmax_iter=300)\n",
        "clf2_en.fit(X_train_tfidf2,\ty_train2)\n",
        "predicted2\t=\tclf2_en.predict(X_dev_tfidf2)\n",
        "# print(\"Predicted\ttask2\\t-->\\t\",\tpredicted2)\n",
        "# print(\"GroundTruth\ttask2\\t-->\\t\",\ty2_dev)\n",
        "#Computing\tthe\tmacro-average\tF1\tfor\tthe\tsecond\tsubtask\n",
        "f1_macro\t=\tf1_score(y_dev2,\tpredicted2,\taverage='macro')\n",
        "print(\"Subtask 2\")\n",
        "print(f\"F1-score\t(Macro-Averaged):\t{f1_macro}\")\n",
        "#Obtainig\ta\tdetailed\tclassification\treport\n",
        "report\t=\tclassification_report(y_dev2, predicted2,\tdigits=4)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGSps7uXT1-O",
        "outputId": "61aa2519-251b-483f-bd6a-acd9f86c83f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 1\n",
            "F1-score\t(Positive\tClass):\t0.6049046321525886\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6937    0.7520    0.7217       250\n",
            "           1     0.6416    0.5722    0.6049       194\n",
            "\n",
            "    accuracy                         0.6734       444\n",
            "   macro avg     0.6677    0.6621    0.6633       444\n",
            "weighted avg     0.6710    0.6734    0.6707       444\n",
            "\n",
            "Subtask 2\n",
            "F1-score\t(Macro-Averaged):\t0.4642297695075139\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6759    0.8588    0.7565        85\n",
            "           1     0.4000    0.2143    0.2791        28\n",
            "           2     0.4348    0.3030    0.3571        33\n",
            "\n",
            "    accuracy                         0.6096       146\n",
            "   macro avg     0.5036    0.4587    0.4642       146\n",
            "weighted avg     0.5685    0.6096    0.5747       146\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNxPsgOwMWg4"
      },
      "source": [
        "## Show Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ref - y_true\n",
        "#pred - y_pred"
      ],
      "metadata": {
        "id": "RYW0Q0jdni1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWjEKmSpMWg4",
        "outputId": "ff02cfe7-6d99-4a36-b9c1-3ede7c6f751a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SUBTASK 1\n",
            "F1-score (Positive Class): 0.7192982456140351\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7601    0.9000    0.8242       250\n",
            "           1     0.8311    0.6340    0.7193       194\n",
            "\n",
            "    accuracy                         0.7838       444\n",
            "   macro avg     0.7956    0.7670    0.7717       444\n",
            "weighted avg     0.7911    0.7838    0.7784       444\n",
            "\n",
            "\n",
            "SUBTASK 1\n",
            "F1-score (Positive Class): 0.7333333333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7944    0.7880    0.7912       250\n",
            "           1     0.7296    0.7371    0.7333       194\n",
            "\n",
            "    accuracy                         0.7658       444\n",
            "   macro avg     0.7620    0.7626    0.7622       444\n",
            "weighted avg     0.7661    0.7658    0.7659       444\n",
            "\n",
            "\n",
            "SUBTASK 2\n",
            "F1-score (Macro-Averaged): 0.285668928840735\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5986    1.0000    0.7489        85\n",
            "           1     0.0000    0.0000    0.0000        28\n",
            "           2     0.5000    0.0606    0.1081        33\n",
            "\n",
            "    accuracy                         0.5959       146\n",
            "   macro avg     0.3662    0.3535    0.2857       146\n",
            "weighted avg     0.4615    0.5959    0.4604       146\n",
            "\n",
            "\n",
            "SUBTASK 2\n",
            "F1-score (Macro-Averaged): 0.44241758241758244\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7010    0.8000    0.7473        85\n",
            "           1     0.3182    0.2500    0.2800        28\n",
            "           2     0.3333    0.2727    0.3000        33\n",
            "\n",
            "    accuracy                         0.5753       146\n",
            "   macro avg     0.4508    0.4409    0.4424       146\n",
            "weighted avg     0.5445    0.5753    0.5566       146\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 1 TASK1 and TASK2 FOR ENGLISH\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "def show_subtask1(ref, pred):\n",
        "    # Computing the F1 Score for the positive class (sexism), subtask 1\n",
        "    f1_positive = f1_score(ref, pred, pos_label=1, zero_division=0)\n",
        "    # Obtainig a detailed classification report\n",
        "    report = classification_report(ref, pred, digits=4, zero_division=0)\n",
        "    print(\"\\nSUBTASK 1\")\n",
        "    print(f\"F1-score (Positive Class): {f1_positive}\")\n",
        "    print(report)\n",
        "\n",
        "def show_subtask2(ref, pred):\n",
        "    # Computing the macro-average F1 for the second subtask\n",
        "    f1_macro = f1_score(ref, pred, average='macro', zero_division=0)\n",
        "    # Obtainig a detailed classification report\n",
        "    report = classification_report(y2_dev,predicted2, digits=4, zero_division=0)\n",
        "    print(\"\\nSUBTASK 2\")\n",
        "    print(f\"F1-score (Macro-Averaged): {f1_macro}\")\n",
        "    print(report)\n",
        "\n",
        "\n",
        "###########################SUBTASK 1##################################\n",
        "\n",
        "# COMPLETE\n",
        "\n",
        "###########################SUBTASK 2##################################\n",
        "\n",
        "# COMPLETE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDVSV3VRMWg5"
      },
      "source": [
        "# SPANISH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH8DpH_hMWg5"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAi107KJMWg5"
      },
      "outputs": [],
      "source": [
        "# COMPLETE IF YOU WANT TO DO TEXT PREPROCESSING\n",
        "# Already made above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_p9B12jMWg6"
      },
      "source": [
        "##¬†Tweet representations (Feature extraction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stopwordsiso"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDbjGADnYV3v",
        "outputId": "85b95406-9998-4fb6-ca5d-cc91388b09fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stopwordsiso\n",
            "  Downloading stopwordsiso-0.6.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/73.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stopwordsiso\n",
            "Successfully installed stopwordsiso-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from stopwordsiso import stopwords  # Install via: pip install stopwordsiso\n",
        "spanish_stopwords = stopwords(\"es\")  # Get Spanish stop words"
      ],
      "metadata": {
        "id": "pRyoILiAYNfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4v9TDLeMWg6"
      },
      "outputs": [],
      "source": [
        "# Obtaining a representation for the train and dev subsets in both tasks\n",
        "\n",
        "# COMPLETE\n",
        "# Convert labels to numerical format\n",
        "label_encoder3 = LabelEncoder()\n",
        "y_train3 = label_encoder3.fit_transform(SpTrainTask1[2])  # Binary labels\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer3 = TfidfVectorizer(stop_words=list(spanish_stopwords),max_features=5000)\n",
        "X_train_tfidf3 = vectorizer3.fit_transform(SpTrainTask1_cleaned)  # Training text data\n",
        "\n",
        "X_dev_tfidf3 = vectorizer3.transform(SpDevTask1_cleaned)\n",
        "y_dev3 = label_encoder3.transform(SpDevTask1[2])\n",
        "\n",
        "# Convert labels to numerical format\n",
        "label_encoder4 = LabelEncoder()\n",
        "y_train4 = label_encoder4.fit_transform(SpTrainTask2[2])  # Binary labels\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer4 = TfidfVectorizer(stop_words=list(spanish_stopwords), max_features=5000)\n",
        "X_train_tfidf4 = vectorizer4.fit_transform(SpTrainTask2_cleaned)  # Training text data\n",
        "\n",
        "X_dev_tfidf4 = vectorizer4.transform(SpDevTask2_cleaned)\n",
        "y_dev4 = label_encoder4.transform(SpDevTask2[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p97o49V0MWg6"
      },
      "source": [
        "## Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuLRJm9oMWg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48258526-5a14-48b1-f822-0207653e1a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 1\n",
            "F1-score (Positive\tClass):\t0.7551867219917012\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.83      0.76       229\n",
            "           1       0.82      0.70      0.76       261\n",
            "\n",
            "    accuracy                           0.76       490\n",
            "   macro avg       0.76      0.76      0.76       490\n",
            "weighted avg       0.77      0.76      0.76       490\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#¬†COMPLETE\n",
        "\n",
        "lr3 = LogisticRegression()\n",
        "lr3.fit(X_train_tfidf3, y_train3)\n",
        "\n",
        "y_pred3 = lr3.predict(X_dev_tfidf3)\n",
        "\n",
        "f1_positive\t= f1_score(y_dev3, y_pred3, pos_label=1)\n",
        "print(\"Subtask 1\")\n",
        "print(f\"F1-score (Positive\tClass):\t{f1_positive}\")\n",
        "print(classification_report(y_dev3, y_pred3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr4 = LogisticRegression()\n",
        "lr4.fit(X_train_tfidf4, y_train4)\n",
        "\n",
        "y_pred4 = lr4.predict(X_dev_tfidf4)\n",
        "\n",
        "f1_positive\t= f1_score(y_dev4, y_pred4, average='macro')\n",
        "print(\"Subtask 2\")\n",
        "print(f\"F1-score (Positive\tClass):\t{f1_positive}\")\n",
        "print(classification_report(y_dev4, y_pred4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqGlF5YL3lcA",
        "outputId": "7157cdb7-6138-4353-fcb8-72678c231e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 2\n",
            "F1-score (Positive\tClass):\t0.34456992283079235\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.99      0.75       116\n",
            "           1       0.67      0.12      0.20        51\n",
            "           2       0.33      0.05      0.09        40\n",
            "\n",
            "    accuracy                           0.59       207\n",
            "   macro avg       0.53      0.39      0.34       207\n",
            "weighted avg       0.56      0.59      0.48       207\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################SUBTASK\t1##################################\n",
        "#Using\tthe\tSVC\twith\tdefault\tsettings\n",
        "clf1_en\t= svm.SVC()\n",
        "clf1_en.fit(X_train_tfidf3, y_train3)\n",
        "predicted1 = clf1_en.predict(X_dev_tfidf3)\n",
        "# print(\"Predicted task1\\t-->\\t\",\tpredicted1)\n",
        "# print(\"GroundTruth task1\\t-->\\t\", y_dev)\n",
        "#Computing\tthe\tF1\tScore\tfor\tthe\tpositive\tclass\t(sexism),\tsubtask\t1\n",
        "print(\"Subtask 1\")\n",
        "f1_positive\t= f1_score(y_dev3,\tpredicted1,\tpos_label=1)\n",
        "print(f\"F1-score (Positive\tClass):\t{f1_positive}\")\n",
        "#Obtainig a detailed classification\treport\n",
        "report\t= classification_report(y_dev3, predicted1, digits=4)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wle7WI75JOX",
        "outputId": "606cb976-cb52-4d4f-eeaa-ac90a5b5470d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 1\n",
            "F1-score (Positive\tClass):\t0.7436974789915967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6945    0.8341    0.7579       229\n",
            "           1     0.8233    0.6782    0.7437       261\n",
            "\n",
            "    accuracy                         0.7510       490\n",
            "   macro avg     0.7589    0.7561    0.7508       490\n",
            "weighted avg     0.7631    0.7510    0.7504       490\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf2_en\t= svm.SVC()\n",
        "clf2_en.fit(X_train_tfidf4, y_train4)\n",
        "predicted2 = clf2_en.predict(X_dev_tfidf4)\n",
        "# print(\"Predicted task2\\t-->\\t\",\tpredicted2)\n",
        "# print(\"GroundTruth task2\\t-->\\t\", EnDevTask2[2])\n",
        "#Computing\tthe\tmacro-average F1 for the second\tsubtask\n",
        "f1_macro = f1_score(y_dev4, predicted2,\taverage='macro')\n",
        "print(\"Subtask 2\")\n",
        "print(f\"F1-score (Macro-Averaged): {f1_macro}\")\n",
        "#Obtainig a detailed classification\treport\n",
        "report = classification_report(y_dev4, predicted2, digits=4)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id7zcXV_5KAG",
        "outputId": "f6dd50ec-f7a6-49b3-edf2-307266090f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 2\n",
            "F1-score (Macro-Averaged): 0.2960966357192772\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5743    1.0000    0.7296       116\n",
            "           1     1.0000    0.0588    0.1111        51\n",
            "           2     0.5000    0.0250    0.0476        40\n",
            "\n",
            "    accuracy                         0.5797       207\n",
            "   macro avg     0.6914    0.3613    0.2961       207\n",
            "weighted avg     0.6648    0.5797    0.4454       207\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network\timport MLPClassifier\n",
        "from sklearn.metrics\timport f1_score,\tclassification_report\n",
        "###########################SUBTASK\t1##################################\n",
        "#Using\tthe\tMLP\twith\tdefault\tsettings\n",
        "clf1_en\t=\tMLPClassifier(random_state=1,\tmax_iter=300)\n",
        "clf1_en.fit(X_train_tfidf3,\ty_train3)\n",
        "predicted1\t=\tclf1_en.predict(X_dev_tfidf3)\n",
        "# print(\"Predicted\ttask1\\t-->\\t\",\tpredicted1)\n",
        "# print(\"GroundTruth\ttask1\\t-->\\t\",\ty1_dev)\n",
        "#Computing\tthe\tF1\tScore\tfor\tthe\tpositive\tclass\t(sexism),\tsubtask\t1\n",
        "f1_positive\t=\tf1_score(y_dev3,\tpredicted1,\tpos_label=1)\n",
        "print(\"Subtask 1\")\n",
        "print(f\"F1-score\t(Positive\tClass):\t{f1_positive}\")\n",
        "#Obtainig\ta\tdetailed\tclassification\treport\n",
        "report\t=\tclassification_report(y_dev3, predicted1,\tdigits=4)\n",
        "print(report)\n",
        "###########################SUBTASK\t2##################################\n",
        "clf2_en\t=\tMLPClassifier(random_state=1,\tmax_iter=300)\n",
        "clf2_en.fit(X_train_tfidf4,\ty_train4)\n",
        "predicted2\t=\tclf2_en.predict(X_dev_tfidf4)\n",
        "# print(\"Predicted\ttask2\\t-->\\t\",\tpredicted2)\n",
        "# print(\"GroundTruth\ttask2\\t-->\\t\",\ty2_dev)\n",
        "#Computing\tthe\tmacro-average\tF1\tfor\tthe\tsecond\tsubtask\n",
        "f1_macro\t=\tf1_score(y_dev4,\tpredicted2,\taverage='macro')\n",
        "print(\"Subtask 2\")\n",
        "print(f\"F1-score\t(Macro-Averaged):\t{f1_macro}\")\n",
        "#Obtainig\ta\tdetailed\tclassification\treport\n",
        "report\t=\tclassification_report(y_dev4, predicted2,\tdigits=4)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QBpVGEY6ZOm",
        "outputId": "0fbb1512-89e4-4679-a1f0-f084b2cfe378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtask 1\n",
            "F1-score\t(Positive\tClass):\t0.6965376782077393\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6538    0.7424    0.6953       229\n",
            "           1     0.7435    0.6552    0.6965       261\n",
            "\n",
            "    accuracy                         0.6959       490\n",
            "   macro avg     0.6987    0.6988    0.6959       490\n",
            "weighted avg     0.7016    0.6959    0.6960       490\n",
            "\n",
            "Subtask 2\n",
            "F1-score\t(Macro-Averaged):\t0.46408250355618774\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6689    0.8534    0.7500       116\n",
            "           1     0.3600    0.1765    0.2368        51\n",
            "           2     0.4412    0.3750    0.4054        40\n",
            "\n",
            "    accuracy                         0.5942       207\n",
            "   macro avg     0.4900    0.4683    0.4641       207\n",
            "weighted avg     0.5488    0.5942    0.5570       207\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report\n",
        "\n",
        "## 1. Preprocessing Steps\n",
        "To ensure high-quality input for the classification models, several preprocessing techniques were applied to the tweets:\n",
        "- **Removal of Unnecessary Elements:** Hashtags, mentions, URLs, special characters, and emojis were removed.\n",
        "- **Text Normalization:** Extra spaces were eliminated, and all text was converted to lowercase to maintain uniformity.\n",
        "- **Stopwords Removal:** English and Spanish stopwords were removed to reduce noise and improve model focus on meaningful words.\n",
        "\n",
        "## 2. Text Representations\n",
        "To convert text data into numerical format, **TF-IDF (Term Frequency-Inverse Document Frequency)** was used:\n",
        "- **Feature Extraction:** The TF-IDF vectorizer was applied with `max_features=5000` to limit vocabulary size.\n",
        "- **Stopwords Handling:** Both English and Spanish stopwords were excluded.\n",
        "- **Normalization:** Text was lowercased to ensure case-insensitive tokenization.\n",
        "\n",
        "## 3. Classification Models and Hyperparameters\n",
        "Three machine learning models were used for training and evaluation:\n",
        "\n",
        "### Binary Classification (Sexist vs. Non-Sexist Tweets)\n",
        "#### Support Vector Machine (SVM)\n",
        "- **F1-score:** 0.69 (English), 0.73 (Spanish)\n",
        "\n",
        "#### Logistic Regression\n",
        "- **F1-score:** 0.75 (Spanish), 0.68 (English)\n",
        "\n",
        "#### Multi-layer Perceptron (MLP) Neural Network\n",
        "- **F1-score:** 0.60 (English), 0.69 (Spanish)\n",
        "\n",
        "### Multiclass Classification (Sexist Tweet Categorization: DIRECT, REPORTED, JUDGEMENTAL)\n",
        "#### Multi-layer Perceptron (MLP) Neural Network\n",
        "- **F1-score:** 0.46 (English & Spanish)\n",
        "\n",
        "#### Support Vector Machine (SVM)\n",
        "- **F1-score:** Below 0.40\n",
        "\n",
        "## 4. Observations and Conclusions\n",
        "- **Binary classification achieved better performance** than multiclass classification, especially for Spanish tweets.\n",
        "- **SVM performed best for English binary classification**, while **logistic regression was best for Spanish binary classification**.\n",
        "- **MLP outperformed other models in multiclass classification**, but scores were generally low (~0.46 F1-score), indicating the difficulty in distinguishing between sexist categories.\n",
        "- **Future Improvements:** Using deep learning models like **BERT embeddings** instead of TF-IDF could enhance contextual understanding and improve classification performance.\n",
        "\n",
        "### Final Conclusion\n",
        "Logistic Regression and SVM were the most effective for binary classification, while MLP worked best for multiclass classification, although overall multiclass performance remained low.\n",
        "\n"
      ],
      "metadata": {
        "id": "VCZHYrLL-A2D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRRkOLz-MWg6"
      },
      "source": [
        "## Show Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SJ6V0VhkMWg6",
        "outputId": "4758b3bb-886d-4590-e441-7f8cdff7b5e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SUBTASK 1\n",
            "F1-score (Positive Class): 0.7370600414078675\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6903    0.8079    0.7445       229\n",
            "           1     0.8018    0.6820    0.7371       261\n",
            "\n",
            "    accuracy                         0.7408       490\n",
            "   macro avg     0.7461    0.7449    0.7408       490\n",
            "weighted avg     0.7497    0.7408    0.7405       490\n",
            "\n",
            "\n",
            "SUBTASK 1\n",
            "F1-score (Positive Class): 0.7613412228796844\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7213    0.7686    0.7442       229\n",
            "           1     0.7846    0.7395    0.7613       261\n",
            "\n",
            "    accuracy                         0.7531       490\n",
            "   macro avg     0.7529    0.7540    0.7528       490\n",
            "weighted avg     0.7550    0.7531    0.7533       490\n",
            "\n",
            "\n",
            "SUBTASK 2\n",
            "F1-score (Macro-Averaged): 0.27065897065897065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5779    0.9914    0.7302       116\n",
            "           1     0.2500    0.0196    0.0364        51\n",
            "           2     0.2500    0.0250    0.0455        40\n",
            "\n",
            "    accuracy                         0.5652       207\n",
            "   macro avg     0.3593    0.3453    0.2707       207\n",
            "weighted avg     0.4337    0.5652    0.4269       207\n",
            "\n",
            "\n",
            "SUBTASK 2\n",
            "F1-score (Macro-Averaged): 0.48030423280423284\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7315    0.6810    0.7054       116\n",
            "           1     0.3878    0.3725    0.3800        51\n",
            "           2     0.3200    0.4000    0.3556        40\n",
            "\n",
            "    accuracy                         0.5507       207\n",
            "   macro avg     0.4797    0.4845    0.4803       207\n",
            "weighted avg     0.5673    0.5507    0.5576       207\n",
            "\n"
          ]
        }
      ],
      "source": [
        "###########################SUBTASK 1##################################\n",
        "\n",
        "# COMPLETE\n",
        "\n",
        "###########################SUBTASK 2##################################\n",
        "\n",
        "# COMPLETE\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}